# üéØ Guide Mod√®les Recommand√©s

## Profils de Configuration

### üì± Profil L√©ger (8GB GPU / CPU)

**Mod√®les:**
```bash
ollama pull mistral:7b
```

**Configuration `config/settings.py`:**
```python
AGENT_MODELS = {
    "architect": "mistral:7b",
    "developer": "mistral:7b",
    "reviewer": "mistral:7b",
    "security": "mistral:7b",
    "tester": "mistral:7b",
    "documentation": "mistral:7b"
}
```

**Caract√©ristiques:**
- ‚úÖ Rapide (~1-2s par g√©n√©ration)
- ‚úÖ Faible empreinte m√©moire
- ‚úÖ Qualit√© acceptable
- ‚ùå Peut manquer nuances

**Temps estim√©:** 3-5 min par it√©ration


### üíª Profil Moyen (16GB GPU)

**Mod√®les:**
```bash
ollama pull mistral:7b
ollama pull neural-chat:7b
ollama pull codellama:13b
```

**Configuration optimale:**
```python
AGENT_MODELS = {
    "architect": "neural-chat:7b",      # Design cr√©atif
    "developer": "codellama:13b",       # Code forte
    "reviewer": "neural-chat:7b",       # Analyse g√©n√©rale
    "security": "mistral:7b",          # Rapide, l√©ger
    "tester": "codellama:13b",         # Syntaxe tests
    "documentation": "mistral:7b"      # Rapidit√©
}
```

**Caract√©ristiques:**
- ‚úÖ Bon √©quilibre qualit√©/vitesse
- ‚úÖ Qualit√© code tr√®s bonne
- ‚úÖ M√©moire g√©rable
- ‚ö†Ô∏è  ~5-10s par g√©n√©ration

**Temps estim√©:** 8-15 min par it√©ration


### üöÄ Profil Performance (24GB+ GPU)

**Mod√®les:**
```bash
ollama pull codellama:34b
ollama pull deepseek-coder:33b
ollama pull qwen2.5-coder:32b
ollama pull llama2:13b
ollama pull phind-coder:34b
```

**Configuration optimale:**
```python
AGENT_MODELS = {
    "architect": "codellama:34b",       # Excellente conception
    "developer": "deepseek-coder:33b",  # Code tr√®s bon
    "reviewer": "qwen2.5-coder:32b",    # Excellente analyse
    "security": "llama2:13b",           # S√©curit√© forte
    "tester": "phind-coder:34b",        # Tests excellents
    "documentation": "mistral:7b"       # Doc rapide
}
```

**Caract√©ristiques:**
- ‚úÖ‚úÖ Qualit√© exceptionnelle
- ‚úÖ Code tr√®s bon syntaxiquement
- ‚ö†Ô∏è  Lent (~20-30s par g√©n√©ration)
- ‚ö†Ô∏è  Tr√®s gourmand m√©moire

**Temps estim√©:** 30-60 min par it√©ration


## Mod√®les Sp√©cialis√©s Recommand√©s

### üë®‚Äçüíº Architecture
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| CodeLLaMA:34b | 19GB | üèÜ Meilleur design patterns |
| Mistral:7b | 4GB | Rapide, acceptable |
| Neural-Chat:7b | 4GB | Bon √©quilibre |

### üë®‚Äçüíª D√©veloppement
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| DeepSeek-Coder:33b | 19GB | üèÜ Meilleur code |
| CodeLLaMA:34b | 19GB | Tr√®s bon |
| Phind-Coder:34b | 19GB | Excellent |

### üîç Review/Qualit√©
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| Qwen2.5-Coder:32b | 19GB | üèÜ Excellente analyse |
| CodeLLaMA:34b | 19GB | Tr√®s bon |
| Neural-Chat:7b | 4GB | Acceptable |

### üîí S√©curit√©
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| Llama2:13b | 7GB | üèÜ Bon balance |
| Mistral:7b | 4GB | Rapide |
| Neural-Chat:7b | 4GB | Alternative |

### ‚úÖ Testing
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| Phind-Coder:34b | 19GB | üèÜ Excellents tests |
| CodeLLaMA:34b | 19GB | Tr√®s bon |
| DeepSeek-Coder:33b | 19GB | Bon |

### üìö Documentation
| Mod√®le | Taille | Notes |
|--------|--------|-------|
| Mistral:7b | 4GB | üèÜ Rapide et clair |
| Neural-Chat:7b | 4GB | Bon |
| LLaMA2:7b | 4GB | Acceptable |


## S√©lection Automatique

Si tu es ind√©cis, utilise ce profil "intelligent":

```python
# config/settings.py
import os
import psutil

def get_available_gpu_memory():
    """D√©tecte la m√©moire GPU disponible"""
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / 1e9
    except:
        pass
    return 0

gpu_mem = get_available_gpu_memory()

if gpu_mem < 10:  # < 10GB
    PROFILE = "LIGHT"
elif gpu_mem < 20:  # 10-20GB
    PROFILE = "MEDIUM"
else:  # > 20GB
    PROFILE = "HEAVY"

# Appliquer le profil appropri√©
```


## Installation Optimis√©e

### T√©l√©charger avec script

**Windows:**
```powershell
# Profil l√©ger
ollama pull mistral:7b

# Profil moyen
ollama pull mistral:7b; ollama pull neural-chat:7b; ollama pull codellama:13b

# Profil complet
ollama pull codellama:34b; ollama pull deepseek-coder:33b; ollama pull qwen2.5-coder:32b; ollama pull llama2:13b; ollama pull phind-coder:34b
```

**Linux/Mac:**
```bash
# Profil moyen
for model in mistral:7b neural-chat:7b codellama:13b; do
    ollama pull $model
done
```


## Benchmark Exemple

Temps pour g√©n√©rer une API simple (sur RTX 4090):

| Mod√®le | Architecture | Code | Review | Tests | Total |
|--------|--------------|------|--------|-------|-------|
| mistral:7b | 3s | 4s | 3s | 4s | 14s/iter |
| codellama:13b | 5s | 7s | 5s | 6s | 23s/iter |
| codellama:34b | 12s | 15s | 12s | 14s | 53s/iter |


## Conseil pour le Choix

```
Ai-je plus de 24GB GPU?
  ‚Üí OUI: Utilise profil HEAVY (meilleure qualit√©)
  ‚Üí NON
  
Ai-je plus de 16GB GPU?
  ‚Üí OUI: Utilise profil MEDIUM (bon √©quilibre)
  ‚Üí NON
  
Ai-je plus de 8GB GPU?
  ‚Üí OUI: Utilise profil LIGHT (rapide)
  ‚Üí NON: Utilise CPU uniquement (lent mais possible)
```

## Troubleshooting Mod√®les

**Mod√®le qui ne t√©l√©charge pas:**
```bash
ollama pull mistral:latest --insecure
```

**V√©rifier t√©l√©chargement:**
```bash
ollama list
```

**Supprimer mod√®le:**
```bash
ollama rm mistral:latest
```

**Voir espace disque:**
```bash
ollama show mistral:latest
```

---

üí° **Conseil:** Commencez avec `mistral:7b` - c'est rapide et suffit pour tester!
