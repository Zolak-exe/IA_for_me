# ğŸ†˜ Guide DÃ©pannage Complet

## âŒ Erreurs Communes et Solutions

### 1. "Ollama non disponible"

```
âŒ Impossible de se connecter Ã  Ollama
   VÃ©rifiez que Ollama est running sur http://localhost:11434
```

**Causes possibles:**
- Ollama n'est pas lancÃ©
- Port 11434 bloquÃ©/utilisÃ© par autre processus
- Ollama installÃ© mais non configurÃ©

**Solutions:**

```bash
# 1. VÃ©rifier si Ollama est lancÃ©
curl http://localhost:11434/api/tags

# 2. Si erreur, lancer Ollama
ollama serve

# 3. Sur Windows, vÃ©rifier le service
Get-Service Ollama

# 4. Si port occupÃ©, trouver le processus
netstat -ano | findstr :11434  # Windows
lsof -i :11434                  # Linux/Mac

# 5. RedÃ©marrer Ollama
# Windows: taskkill /PID <PID> /F
# Linux/Mac: kill -9 <PID>
```


### 2. "Aucun modÃ¨le trouvÃ©"

```
âš ï¸  Aucun modÃ¨le trouvÃ©
   TÃ©lÃ©charge au moins un modÃ¨le avec: ollama pull mistral
```

**Causes:**
- Aucun modÃ¨le n'a Ã©tÃ© tÃ©lÃ©chargÃ©
- ModÃ¨les corrompus/mal tÃ©lÃ©chargÃ©s
- RÃ©pertoire modÃ¨les inaccessible

**Solutions:**

```bash
# 1. Lister modÃ¨les disponibles
ollama list

# 2. TÃ©lÃ©charger modÃ¨le (recommandÃ©: mistral)
ollama pull mistral:latest

# 3. Ou choisir autre modÃ¨le
ollama pull neural-chat:7b
ollama pull llama2:7b

# 4. VÃ©rifier espace disque
df -h  # Linux/Mac
Disk Usage  # Windows

# 5. Si corruption, supprimer et rÃ©installer
ollama rm mistral:latest
ollama pull mistral:latest
```


### 3. "Timeout" lors de la gÃ©nÃ©ration

```
Timeout tentative 1/3
Timeout tentative 2/3
âŒ Impossible de gÃ©nÃ©rer aprÃ¨s 3 tentatives
```

**Causes:**
- ModÃ¨le trop gros pour votre GPU
- Trop peu de VRAM disponible
- RÃ©seau lent
- ModÃ¨le trop complexe

**Solutions:**

```python
# config/settings.py
OLLAMA_CONFIG = {
    "timeout": 600,  # Augmenter de 300 Ã  600 (10 min)
}

# OU utiliser un modÃ¨le plus lÃ©ger:
AGENT_MODELS = {
    "architect": "mistral:7b",      # LÃ©ger
    "developer": "mistral:7b",
    # ...
}

# OU rÃ©duire les itÃ©rations:
# main.py --max-iterations 3
```

**VÃ©rifier ressources:**

```bash
# GPU NVIDIA
nvidia-smi

# GPU AMD
rocm-smi

# MÃ©moire gÃ©nÃ©rale
free -h  # Linux/Mac
systeminfo | findstr Memory  # Windows
```


### 4. "Out of Memory" (OOM)

```
CUDA out of memory
RuntimeError: CUDA ran out of memory
```

**Causes:**
- GPU manque de VRAM
- ModÃ¨le trop gros
- Trop d'instances du modÃ¨le chargÃ©es

**Solutions:**

```bash
# 1. VÃ©rifier VRAM disponible
nvidia-smi -q -d Memory | head -n 3

# 2. LibÃ©rer mÃ©moire
# Fermer autres applications
# RedÃ©marrer Ollama et les applications

# 3. Utiliser modÃ¨les plus petits
ollama pull mistral:7b  # 4GB
ollama rm codellama:34b  # Supprimer le gros

# 4. Changer configuration
# config/settings.py -> utiliser mistral partout
```

**Profil mÃ©moire par modÃ¨le:**
- 7B: 4-6 GB VRAM
- 13B: 8-10 GB VRAM
- 34B: 18-24 GB VRAM


### 5. "Type error" ou "JSON decode error"

```
JSONDecodeError: Expecting value
AttributeError: 'NoneType' object has no attribute 'get'
```

**Causes:**
- RÃ©ponse malformÃ©e d'Ollama
- ModÃ¨le pas complÃ¨tement lancÃ©
- ProblÃ¨me rÃ©seau

**Solutions:**

```bash
# 1. VÃ©rifier santÃ© Ollama
curl http://localhost:11434/api/tags -v

# 2. Relancer Ollama
# Ctrl+C pour arrÃªter
# ollama serve pour relancer

# 3. VÃ©rifier modÃ¨le
ollama list
ollama show mistral:latest

# 4. Activer logs dÃ©taillÃ©s
# main.py --verbose

# 5. VÃ©rifier Python version
python --version  # Doit Ãªtre 3.10+
```


### 6. "ModuleNotFoundError"

```
ModuleNotFoundError: No module named 'requests'
```

**Causes:**
- DÃ©pendances non installÃ©es
- Virtualenv pas activÃ©
- Mauvais Python utilisÃ©

**Solutions:**

```bash
# 1. VÃ©rifier virtualenv actif
which python  # Doit indiquer venv/bin/python

# 2. Activer virtualenv
.\venv\Scripts\Activate.ps1  # Windows
source venv/bin/activate     # Linux/Mac

# 3. Installer dÃ©pendances
pip install -r requirements.txt

# 4. VÃ©rifier installation
pip list | grep requests

# 5. RÃ©installer si besoin
pip install --upgrade requests
```


### 7. "Port already in use"

```
OSError: [Errno 48] Address already in use
Address 127.0.0.1:11434 is already in use
```

**Causes:**
- Port 11434 dÃ©jÃ  utilisÃ©
- Ollama en double
- Autre service sur ce port

**Solutions:**

```bash
# 1. Trouver le processus
netstat -ano | findstr :11434  # Windows
lsof -i :11434                  # Linux/Mac

# 2. VÃ©rifier si c'est Ollama
ps aux | grep ollama

# 3. Tuer le processus
taskkill /PID 12345 /F  # Windows (remplacer 12345)
kill -9 12345           # Linux/Mac

# 4. OU changer le port (config/settings.py)
OLLAMA_CONFIG = {
    "base_url": "http://localhost:11435",  # Nouveau port
}
```


### 8. GÃ©nÃ©ration de code vide

```
âœ“ GÃ©nÃ©ration terminÃ©e
   RÃ©ponse: ""
```

**Causes:**
- ModÃ¨le ne rÃ©pond pas
- Prompt trop complexe
- TempÃ©rature trop basse

**Solutions:**

```python
# config/settings.py
GENERATION_PARAMS = {
    "developer": {
        "temperature": 0.7,  # Augmenter de 0.5 Ã  0.7
    }
}

# OU vÃ©rifier le modÃ¨le
# ollama show mistral:latest
# ollama list

# OU activer logs dÃ©taillÃ©s pour voir les prompts
# main.py --verbose

# OU utiliser modÃ¨le diffÃ©rent
AGENT_MODELS = {
    "developer": "neural-chat:7b",  # Essayer autre
}
```


### 9. "Connection refused"

```
ConnectionRefusedError: [Errno 111] Connection refused
```

**Causes:**
- Ollama pas lancÃ©
- URL incorrecte
- Firewall bloque connexion

**Solutions:**

```bash
# 1. Lancer Ollama
ollama serve

# 2. VÃ©rifier URL
# config/settings.py -> "http://localhost:11434"

# 3. Tester connexion manuelle
curl http://localhost:11434/api/tags

# 4. VÃ©rifier firewall
# Windows: Windows Defender Firewall
# Linux: sudo ufw allow 11434
```


### 10. Performance lente

```
Phase 1: Architecture... (30s+ attente)
TrÃ¨s lent lors gÃ©nÃ©ration
```

**Causes:**
- GPU chargÃ©
- ModÃ¨le trop gros
- CPU utilisÃ© au lieu GPU

**Solutions:**

```bash
# 1. VÃ©rifier GPU utilisÃ©
nvidia-smi  # Doit voir CUDA
rocm-smi    # Doit voir ROCm

# 2. LibÃ©rer GPU
# Fermer applications gourmandes

# 3. RÃ©duire taille modÃ¨le
ollama rm codellama:34b
ollama pull mistral:7b

# 4. Augmenter batch size (si config GPU avancÃ©e)

# 5. RÃ©duire nombre itÃ©rations
main.py --max-iterations 3

# 6. Profiling (benchmark)
# Voir MODELS_GUIDE.md
```


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ” DEBUGGING AVANCÃ‰

### Activer tous les logs

```bash
python main.py --requirements "..." --verbose
```

GÃ©nÃ¨re logs dÃ©taillÃ©s dans `system.log`


### Tester composant par composant

```bash
# 1. Test Ollama seul
python test_ollama.py

# 2. Test avec petit modÃ¨le
python example.py

# 3. Test avec dÃ©mo interactive
python demo.py

# 4. Test spÃ©cifique agent
python -c "
from core import OllamaClient
from agents import ArchitectAgent
# ...
"
```


### Profiling d'exÃ©cution

```bash
# Mesurer temps avec verbose
python -m cProfile -s cumtime main.py --requirements "..." --verbose | head -20

# Voir utilisation mÃ©moire
python -m memory_profiler main.py --requirements "..." --verbose
```


### VÃ©rifier les fichiers de log

```bash
# Affichage temps rÃ©el
tail -f system.log

# Rechercher erreurs
grep -i error system.log

# DerniÃ¨res 50 lignes
tail -50 system.log
```


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ’¡ CONSEILS

1. **Commencer simple**
   - Tester d'abord avec 3 itÃ©rations
   - Utiliser mistral:7b
   - Petit projet pour validation

2. **Surveiller ressources**
   - Ouvrir nvidia-smi avant lancement
   - Garder ~30% VRAM libre
   - Fermer applications lourdes

3. **Lire les logs**
   - system.log = meilleure source info
   - --verbose pour plus de dÃ©tails
   - Chercher "Error" ou "Failed"

4. **Backup modÃ¨les**
   - ModÃ¨les tÃ©lÃ©chargÃ©s peuvent Ãªtre volumineux
   - Sauvegarder ~/.ollama aprÃ¨s setup
   - Ne rÃ©installer que si nÃ©cessaire

5. **ExpÃ©rimenter**
   - Essayer diffÃ©rents modÃ¨les
   - Varier tempÃ©rature/paramÃ¨tres
   - Consulter MODELS_GUIDE.md


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“ SUPPORT

Si problÃ¨me persiste:

1. Consulter les logs: `system.log`
2. Relire cette page
3. VÃ©rifier `README.md`
4. Tester `test_ollama.py`
5. Essayer `demo.py` avec projet prÃ©-dÃ©fini

Informations utiles pour dÃ©pannage:
- `python --version`
- `ollama list`
- GPU disponible (`nvidia-smi`)
- OS (Windows/Linux/Mac)
- Messages d'erreur exacts
